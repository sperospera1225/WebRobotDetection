{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_colab.ipynb","private_outputs":true,"provenance":[{"file_id":"1G3VAQQ0YMwYtB-mHy7jVwu6N684O5KsV","timestamp":1638862125992}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PowGq7Pntt5C"},"source":["#***Web robot detection based on Log access Pattern recognition***\n","\n","####17102042 Min-seon Kim\n","####17102062 Yong-Hoon Lee\n"]},{"cell_type":"markdown","metadata":{"id":"AaLyasYliFrX"},"source":["\n","##- Background[1]\n","\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1a1iV7b3dJDqakWtiLRkS7_xNk88m7B0F\" width=\"700\">\n","\n","According to a recent survey, 37.2% of all internet users were robots in 2020. It consists of 13.1% of good bots and 24.1% of bad bots. \n","And bad bots considered malicious usually threaten the security and privacy of web applications and users. \n","\n","In this project, the workflow pipeline which was proposed in the \"*DeepLog: Anomaly Detection and Diagnotics from System Logs*\" was applied to web robot detection by using the URL access pattern.\n","The core assumption of the model in this project is that humans leave log records through URLs of similar related topics, but robots will repeat the standardized pattern regardless of the subject. For that reasons, we can determine the presence or absence of robots by capturing the uri access pattern which has been made by various web bots.\n"]},{"cell_type":"markdown","metadata":{"id":"wH5lqcBGlwWd"},"source":["## - Dataset[2]\n"," \n","<img src=\"https://drive.google.com/uc?id=1CKIBAo0RFxc5P806dWmegGGSBG7Djsil\" width=\"700\">\n","\n","We used the web robot server log open data posted on ZENODE. This dataset includes server logs from search engines in libraries and information centers at the University of Aristotle of Thessaloniki in Greece (http://search.lib.auth.gr/). <br>\n","The search engine allows users to check the availability of books and other researches for digitized materials and scientific publications.\n","<br>\n","<br>\n","\n","\n"," \n","<img src=\"https://drive.google.com/uc?id=1ClJDTdhauvd8jnxOW_JznHo1PQlZXBO0\" width=\"1000\">\n","\n","There are 9 columns in the data : \n","\n","referrer, request, method, resource, bytes, response, ip, useragent, timestamp\n","\n","The data timestamp is from March 1st to March 31 2018 <br>\n","\n","Total request : 4,091,155 requests  <br>\n","Average request per day : 131,973 requests <br>\n","Total unique IP : 27,061  <br>\n","Total unique user-agent : 3,441 <br>\n"]},{"cell_type":"code","metadata":{"id":"OfSx-6a-t84t"},"source":["import pandas as pd\n","import json\n","import numpy as np\n","from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)\n","# load data\n","test_file = pd.read_csv('/gdrive/My Drive/data300000.csv', encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_file.head()"],"metadata":{"id":"MJEm7hwC_vQr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBwo7ERhuEc0"},"source":["## - Data preprocessing\n","<br>\n","\n","### Using column\n","* we only use 5 columns request, response, ip, useragent and timestamp\n","\n","###'Request' column\n","*  we use the first value after the 'get' letter in the request as a representative. The get method contains information in the URL, so the first value, which is the most meaningful information, is used as the representative of the request.\n","\n","* Use Encoder to change 'request' from string to int.\n","\n","### 'Label' column\n","* A label column is added, which means 1 bot means 0 for humans. Label refers to the user agent.\n"]},{"cell_type":"code","metadata":{"id":"0K-Ea68AuEyT"},"source":["# use 5 columns\n","need_col = ['request', 'response', 'ip', 'useragent', 'timestamp']\n","\n","df = test_file[need_col]\n","df = df.dropna(axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHSd6-OtvfBb"},"source":["# timestamp change to datetime\n","df['timestamp'] = pd.to_datetime(df['timestamp'])\n","\n","# request preprocessing\n","df['request'] = df['request'].apply(lambda x: str(x).split()[7].split('/')[1] if (str(x).split()[7].split('/')[0]==\"\") else \"Timeout\")\n","\n","df['request'] = df['request'].apply(lambda x:\"rc4.js?\" if x[:7]==\"rc4.js?\" else x)\n","df['request'] = df['request'].apply(lambda x:\"favicon.ico?\" if x[:12]==\"favicon.ico?\" else x)\n","df['request'] = df['request'].apply(lambda x:\"sitemap-n.xml\" if x[:8]==\"sitemap-\" else x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xOo8_BPxve-x"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","LE = LabelEncoder() # Encoder to change String to Int\n","df['request'] = LE.fit_transform(df['request']) # sql_syntax int encoding\n","df = df.sort_values(by=['timestamp'])\n","\n","# make label column by using useragent\n","df['label'] = df['useragent'].apply(lambda x: 1 if 'bot' in str(x) else (1 if 'crawl' in str(x) else (1 if 'BUbiNG' in str(x) else (1 if 'Bot' in str(x) else (1 if 'Crawl' in str(x) else 0)))))\n","num_classes = len(df['request'].unique())\n","\n","# index reset\n","df.reset_index(drop=True, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"UdtUFPE0Ak7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"51hWL32DnmwU"},"source":["##- Method and Algorithm\n","\n","###Reffered paper [3] <br>\n","I referred to the DeepLog paper published in 2017 at Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. \n","<br>\n","<br>\n","\n","###**Method1 : log key and parameter**\n","DeepLog paper\n","<br>\n","<img src=\"https://drive.google.com/uc?id=1Ha7t3iyY4akuIrguirPpd3airL7czvO5\" width=\"700\">\n","<br>\n","Our method flow\n","<br>\n","<img src=\"https://drive.google.com/uc?id=1xUO9Fgdw83-IfLlP0MEAZT_D1Y44ELiX\" width=\"700\">\n","<br>\n","<br>\n","\n","All log entries were divided by key values and parameters. Key values are log bodies excluding specific values, and parameters are variable values that enter values. By parsing the log entries through the log parser, all logs are divided into log key and parameter value vector. In this step, value of the parameter vector is time difference between the current log and previous log. \n","Two different models are trained with the order of log key values and the sequence of parameter values based on unique ip.\n","When the new log entry generated, the log key is checked against pretrained log key anomaly detection model to see if there’s any anomaly. If not anomaly, it will further check this parameter value vector against parameter value anomaly detection model for that log key to see if there’s any anomaly happens. After checking through two different models, it will discern the ip's identity to access the website. \n","<br>\n","<br>\n","To sum up, in our team project, we define the log entry as the combination of log key value whose column name is “request” and the parameter value for time difference between the current log and previous log.\n","\n","<img src=\"https://drive.google.com/uc?id=1DhahieGUkVKjAkArqL-3UslOjGn-s6Fl\" width=\"700\">\n","<br>\n","<br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"n-8rSqPvsprg"},"source":["###**Method2 : Window size**\n","<img src=\"https://drive.google.com/uc?id=1OteYOLsIphF4jS-Ok_ePCDoiATdbdXkA\" width=\"700\">\n","<br>\n","In our model the window size is one of the most important hyperparameter. The above image is example of window size. In our project, we set window size 10.<br>\n","After selecting the window size as 10, the encoded sequence of the log key and paramter whose length is 10 is generated. We use this sequence data sets with 10 length vector as input for LSTM model and predict the 11th log key and parameter value."]},{"cell_type":"markdown","metadata":{"id":"A5q49JqavvPK"},"source":["# **Detailed Code Explanation**\n","### Method1 : Make unique key and parameter list"]},{"cell_type":"code","metadata":{"id":"EavAbjHit_cE"},"source":["# make unique key and parameter dictionary\n","# Generate the sequence of parameter value of all IP.\n","\n","log_entry = \"ip\"\n","unique_key = df[log_entry].unique()\n","unique_n = len(df[log_entry].unique())\n","\n","# parameter\n","param_bot = dict()\n","param_man = dict()\n","\n","# log key\n","log_key_bot = dict()\n","log_key_man = dict()\n","\n","# make empty dictionary\n","for i in range(unique_n):\n","    param_bot[unique_key[i]] = []\n","\n","for i in range(unique_n):\n","    param_man[unique_key[i]] = []\n","\n","for i in range(unique_n):\n","    log_key_bot[unique_key[i]] = []\n","\n","for i in range(unique_n):\n","    log_key_man[unique_key[i]] = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMmu2rxrh87f"},"source":["# allign parameter (time difference between the current log and previous log, label)\n","# allign log_key (request, label)\n","# label is added to measure the performance of the model\n","for idx in range(len(df)):\n","    if idx%10000==0:\n","        #print(idx)\n","        pass\n","    if df.iloc[idx]['label']==1:\n","        param_bot[df[log_entry][idx]].append((df.iloc[idx]['timestamp']-df.iloc[idx-1]['timestamp'], df.iloc[idx][\"label\"]))\n","    if df.iloc[idx]['label']==0:\n","        param_man[df[log_entry][idx]].append((df.iloc[idx]['timestamp']-df.iloc[idx-1]['timestamp'], df.iloc[idx][\"label\"]))\n","    if df.iloc[idx]['label']==1:\n","        log_key_bot[df[log_entry][idx]].append((df.iloc[idx]['request'], df.iloc[idx][\"label\"]))\n","    if df.iloc[idx]['label']==0:\n","        log_key_man[df[log_entry][idx]].append((df.iloc[idx]['request'], df.iloc[idx][\"label\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WojCQP2nv39v"},"source":["# Make unique key and parameter list\n","# Sequence is according to ip and time difference\n","seq_param_bot = []\n","seq_param_man = []\n","seq_log_key_bot = []\n","seq_log_key_man = []\n","\n","for k in param_bot.keys():\n","    seq_param_bot.append(param_bot[k])\n","\n","for k in param_man.keys():\n","    seq_param_man.append(param_man[k])\n","\n","for k in log_key_bot.keys():\n","    seq_log_key_bot.append(log_key_bot[k])\n","\n","for k in log_key_man.keys():\n","    seq_log_key_man.append(log_key_man[k])\n","\n","seq_param_bot.sort(key=len, reverse=True)\n","seq_param_man.sort(key=len, reverse=True)\n","seq_log_key_bot.sort(key=len, reverse=True)\n","seq_log_key_man.sort(key=len, reverse=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4rCOSprv37Z"},"source":["# delete the sequence smaller than 5\n","idx = len(seq_param_bot)\n","for item in range(len(seq_param_bot)):\n","    if len(seq_param_bot[item]) <= 5:    \n","        idx = item\n","        break\n","\n","seq_param_bot = seq_param_bot[:idx]\n","\n","# delete the sequence smaller than 5\n","idx = len(seq_param_man)\n","for item in range(len(seq_param_man)):\n","    if len(seq_param_man[item]) <= 5:   \n","        break\n","\n","seq_param_man = seq_param_man[:idx]\n","\n","# delete the sequence smaller than 5\n","idx = len(seq_log_key_bot)\n","for item in range(len(seq_log_key_bot)):\n","    if len(seq_log_key_bot[item]) <= 5:\n","        idx = item\n","        break\n","\n","seq_log_key_bot = seq_log_key_bot[:idx]\n","\n","# delete the sequence smaller than 5\n","idx = len(seq_log_key_man)\n","for item in range(len(seq_log_key_man)):\n","    if len(seq_log_key_man[item]) <= 5:\n","        idx = item\n","        break\n","        \n","seq_log_key_man = seq_log_key_man[:idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seq_param_man[0][:10]"],"metadata":{"id":"sTfqvgDKAoeJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seq_log_key_bot[5][:10]"],"metadata":{"id":"-zYu_LF3ApjO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rgAUNgAkwN3e"},"source":["### Divide train set and validation set per each ip. the ratio is 0.8 and 0.2\n","<img src=\"https://drive.google.com/uc?id=1mAfFAzyioY-DZLq-oCm6fjauw8xh-nlF\" width=\"700\">\n","<br>"]},{"cell_type":"code","metadata":{"id":"g1LNuCN1v35K"},"source":["nums = 0\n","for seq in seq_log_key_bot:\n","    nums += len(seq)\n","\n","ratio = 0.8\n","train_num = int(nums*ratio)\n","tmp = 0\n","# log key train and log key validation \n","key_train = []\n","key_valid = []\n","\n","for seq in seq_log_key_bot:\n","    tmp = len(seq)\n","    idx = int(ratio * tmp)\n","    key_train.append(seq[:idx])\n","    key_valid.append(seq[idx:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eO5_hk2av3zi"},"source":["ratio = 0.8\n","train_num = int(nums*ratio)\n","tmp = 0\n","# parameter train and parameter validation\n","param_train = []\n","param_valid = []\n","\n","for seq in seq_param_bot:\n","    tmp = len(seq)\n","    idx = int(ratio * tmp)\n","    param_train.append(seq[:idx])\n","    param_valid.append(seq[idx:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GGJ6m95nwcyY"},"source":["###Method2 : based on window size 10, make 2 dimensional list"]},{"cell_type":"code","metadata":{"id":"_GLDDHFnwlf_"},"source":["import numpy as np\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import time\n","import tensorflow as tf\n","import tensorflow as tf\n","import os\n","# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n","\n","# make list by pre-set window size\n","def generate(name, window_size):\n","    num_sessions = 0\n","    inputs = []\n","    outputs = []\n","\n","    for line in name:\n","        num_sessions += 1\n","        for i in range(len(line) - window_size):\n","            inputs_tmp = []\n","            for j in line[i:i + window_size]:\n","                inputs_tmp.append(j[0])\n","            inputs.append(inputs_tmp)\n","            outputs.append(line[i + window_size][0])\n","    return inputs, outputs\n","\n","\n","# window size is 10\n","window_size = 10\n","num_classes = len(df['request'].unique())\n","\n","TP = 0\n","FP = 0\n","n_candidates = 10  # top n probability for predicted result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NybrUNoUwld0"},"source":["# generate the train data and validation data\n","X, Y = generate(key_train, window_size)\n","X = np.reshape(X, (len(X), window_size, 1))\n","Y = to_categorical(Y, num_classes)\n","\n","X_valid, Y_valid = generate(key_valid, window_size)\n","X_valid = np.reshape(X_valid, (len(X_valid), window_size, 1))\n","Y_valid = to_categorical(Y_valid, num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X[5][:5]"],"metadata":{"id":"ZaO8XT8FAtgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y[5][:5]"],"metadata":{"id":"RlKQWWqKAtxf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7BQV_E8wtwr"},"source":["###This is parameter case (using window size)\n","<img src=\"https://drive.google.com/uc?id=1xoTBCl1oRs4bw30pmvO2zcRQ3cfaB0N_\" width=\"700\">\n","<br>"]},{"cell_type":"code","source":["# the function for generate the train data set for parameter model\n","def generate_param(name, window_size):\n","    num_sessions = 0\n","    inputs = []\n","    outputs = []\n","\n","    for line in name:\n","        num_sessions += 1\n","        for i in range(len(line) - window_size):\n","            inputs_tmp = []\n","            for j in line[i:i + window_size]:\n","                inputs_tmp.append(j[0])\n","            inputs.append(inputs_tmp)\n","            outputs.append(line[i + window_size][0])\n","    return inputs, outputs"],"metadata":{"id":"XpiFK9kzk0oR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate the train data and validation data\n","X_p, Y_p = generate_param(param_train, window_size)\n","X_p_valid, Y_p_valid = generate_param(param_valid, window_size)"],"metadata":{"id":"c7U2sF5Ok2Gi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encode the time difference based on the quotient divided by 10.\n","for i in range(len(X_p)):\n","    for j in range(len(X_p[i])):\n","        X_p[i][j] = int(X_p[i][j].total_seconds())//10\n","\n","for i in range(len(Y_p)):\n","    Y_p[i] = int(Y_p[i].total_seconds())//10\n","\n","for i in range(len(X_p_valid)):\n","    for j in range(len(X_p_valid[i])):\n","        X_p_valid[i][j] = int(X_p_valid[i][j].total_seconds())//10\n","\n","for i in range(len(Y_p_valid)):\n","    Y_p_valid[i] = int(Y_p_valid[i].total_seconds())//10\n","\n","new_list = []\n","url_set = set()\n","\n","for item in X_p:\n","    if item[2] not in url_set:\n","        url_set.add(item[2])\n","        new_list.append(item[2])\n","    else:\n","        pass\n","\n","for item in Y_p:\n","    if item not in url_set:\n","        url_set.add(item)\n","        new_list.append(item)\n","    else:\n","        pass"],"metadata":{"id":"rC3uBzJM-PFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_params = len(new_list)\n","num_params = 30  # from 0 seconds to 300 seconds\n","\n","X_p = np.array(X_p).reshape(-1,10,1)\n","targets = np.array([Y_p]).reshape(-1)\n","Y_p = np.eye(num_params)[targets]\n","\n","X_p_valid = np.array(X_p_valid).reshape(-1,10,1)\n","targets = np.array([Y_p_valid]).reshape(-1)\n","Y_p_valid = np.eye(num_params)[targets]"],"metadata":{"id":"6ZFn2DFhA0U3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_p[41981]"],"metadata":{"id":"U8NPc8MMAwPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y_p[5]"],"metadata":{"id":"Orgeu2U_Awe5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9akHBwonxBwm"},"source":["## - Make model\n","\n","### Set hyperparameter\n","We set various value of hyperparameter, and through validation, we can find best value of hyperparameter\n","<br>\n","<br>\n","batch_size = 20000<br>\n","optimizer = Adam(lr=3e-4)<br>\n","max epoch_num = 100<br><br>\n","\n","### Using callback\n","**ModelCheckpoint**<br>\n","Only save the model weight when validation loss is improved<br><br>\n","\n","**EarlyStopping**<br>\n","If model has not been improved for 10 epochs, stop training<br><br>\n","\n","\n","**LSTM model with Dense Layer**<br>\n","3 hidden layer for LSTM model<br><br>\n","**Adam optimizer**"]},{"cell_type":"code","metadata":{"id":"UiistBpIxx58"},"source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n","\n","output_size = Y.shape[1]\n","batch_size = 2000\n","optimizer = Adam(lr=3e-4)\n","epoch_num = 100\n","\n","filename = 'checkpoint-epoch-{}-trial02.h5'.format(epoch_num)\n","checkpoint_callback = ModelCheckpoint(filename,             # file name\n","                             monitor='val_accuracy',   # call when val_loss improves\n","                             verbose=1,            # print log\n","                             save_best_only=True,  # only save best value\n","                             mode='auto'          # automatically find best\n","                            )\n","\n","early_stopping = EarlyStopping(monitor='val_accuracy',  # monitoring point (val loss) \n","                              patience=10,         # if val_loss is not improving unitil 10 epoch, end training\n","                              )\n","\n","# There are two options.\n","model = Sequential()\n","model.add(LSTM(512, activation='relu', return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n","model.add(LSTM(256, return_sequences=True))\n","model.add(LSTM(256, return_sequences=False))\n","model.add(Dense(output_size, activation='softmax'))\n","model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n","model.fit(X, Y, epochs=epoch_num ,validation_data=(X_valid, Y_valid), callbacks=[checkpoint_callback, early_stopping], batch_size=batch_size, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#output_size = Y.shape[1]\n","batch_size = 2000\n","optimizer = Adam(lr=3e-4)\n","epoch_num = 100\n","\n","filename = 'checkpoint-epoch-{}-trial02.h5'.format(epoch_num)\n","checkpoint_callback = ModelCheckpoint(filename,             # file name\n","                             monitor='val_accuracy',   # call when val_loss improves\n","                             verbose=1,            # print log\n","                             save_best_only=True,  # only save best value\n","                             mode='auto'          # automatically find best\n","                            )\n","\n","early_stopping = EarlyStopping(monitor='val_accuracy',  # monitoring point (val loss) \n","                              patience=10,         # if val_loss is not improving unitil 10 epoch, end training\n","                              )\n","\n","# the numbrer of classes for parameter value is 30 (one-hot encoded)\n","output_size = 30\n","\n","model2 = Sequential()\n","model2.add(LSTM(128, activation='relu', return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n","model2.add(LSTM(64, return_sequences=True))\n","model2.add(LSTM(32, return_sequences=False))\n","model2.add(Dense(output_size, activation='softmax'))\n","model2.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n","model2.fit(X_p, Y_p, epochs=epoch_num ,validation_data=(X_p_valid, Y_p_valid), callbacks=[checkpoint_callback, early_stopping], batch_size=batch_size, shuffle=True)"],"metadata":{"id":"rTiIdB8OlFdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(seq_param_man)):\n","    for j in range(len(seq_param_man[i])):\n","        seq_param_man[i][j] = list(seq_param_man[i][j])\n","for i in range(len(seq_param_man)):\n","    for j in range(len(seq_param_man[i])):\n","        seq_param_man[i][j][0] = int(seq_param_man[i][j][0].total_seconds())//10"],"metadata":{"id":"UztQPtoVlT4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A function that returns the log key and parameter sequence and the label value\n","def generate_pred(file, window_size):\n","    hdfs = list()\n","    haaa = list()\n","    hhhh = list()\n","\n","    uri = []\n","    time = []\n","    trid = []\n","\n","    for line in file:\n","        uri_tmp = []\n","        trid_tmp = []\n","        time_tmp = []\n","        for i in line:\n","            uri_tmp.append(i[0])\n","            time_tmp.append(int(i[1].total_seconds())//10)\n","            trid_tmp.append(i[2])\n","        uri.append(uri_tmp)\n","        time.append(time_tmp)\n","        trid.append(trid_tmp)\n","    # pad the sequence when shorter than window size\n","    for ln in uri:\n","        line = list(map(lambda n: n - 1, ln))\n","        ln = line + [-2] * (window_size + 1 - len(line))\n","        hdfs.append(tuple(ln))\n","\n","    for ll in trid:\n","        line = list(ll)\n","        ll = line + [-2] * (window_size + 1 - len(line))\n","        hhhh.append(tuple(ll))\n","        \n","    for l in time:\n","        line = list(l)\n","        l = line + [-2] * (window_size + 1 - len(line))\n","        haaa.append(tuple(l))\n","\n","    return hdfs, haaa, hhhh"],"metadata":{"id":"eWTtyYtSlab4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the dictionary for the final test using the log entries from human\n","man = dict()\n","\n","for i in range(unique_n):\n","    man[unique_key[i]] = []\n","\n","for idx in range(len(df)):\n","    if df.iloc[idx]['label']==0:\n","        man[df[log_entry][idx]].append((df.iloc[idx]['request'], df.iloc[idx]['timestamp']-df.iloc[idx-1]['timestamp'], df.iloc[idx][\"label\"]))\n","\n","man_ = []\n","for k in man.keys():\n","    man_.append(man[k])\n","\n","man_.sort(key=len, reverse=True)"],"metadata":{"id":"w2R1eNS2MFdD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# delete the sequence shorter than 5\n","idx = len(man_)\n","for item in range(len(man_)):\n","    if len(man_[item]) <= 5:\n","        idx = item\n","        break\n","\n","man_ = man_[:idx]"],"metadata":{"id":"MPWFZXegQw2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_key_normal_loader, test_normal_loader, y_test = generate_pred(man_, window_size)"],"metadata":{"id":"ooeeQcaAQyI4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C7DbwJCFyMue"},"source":["## Performance evaluation result\n","<img src=\"https://drive.google.com/uc?id=1sSPk_5XrWVt9nYhBFmPvFgcTd-L31PSz\" width=\"700\">\n","<br>"]},{"cell_type":"code","metadata":{"id":"3WpsGepHyRZo"},"source":["from tqdm import tqdm\n","from tensorflow import keras \n","from tensorflow.keras.activations import softmax\n","total = 0\n","correct = 0\n","fail = 0\n","human_count = 0 # count the number of human\n","proba = []\n","y_labeled = []\n","start_time = time.time()\n","\n","# predict for the entire sequence\n","for line, line2, y in tqdm(zip(test_key_normal_loader, test_normal_loader, y_test)):\n","    compare_int = 0  # how many times the prediction false in the sequence (for unique IP)\n","    for i in range(len(line) - window_size):\n","        # the 0.7*length of the sequence\n","        # compare this variable with the the compare_int variable\n","        compare = int(len(line) * 0.3)\n","        seq = line[i:i + window_size]\n","        seq_param = line2[i:i + window_size]\n","        label = line[i + window_size]\n","        label_param = line2[i + window_size]\n","        trid = y[i + window_size]\n","        if label == -2:\n","            continue\n","\n","        X = np.reshape(seq, (1, window_size, 1))\n","        X = X / float(num_classes)\n","        Y = to_categorical(label, num_classes)\n","        prediction = model.predict(X, verbose=0)\n","\n","        predicted = prediction.argsort()[0][::-1][: n_candidates]\n","        y_pred = prediction\n","\n","        proba.append(y_pred)\n","        total += 1\n","\n","        if np.argmax(Y) in prediction.argsort()[0][::-1][: n_candidates]:\n","            Xp = np.reshape(seq_param, (1, window_size, 1))\n","            Xp = Xp / float(30)\n","            Yp = to_categorical(label_param, 30)\n","            \n","            prediction2 = model2.predict(Xp, verbose=0)\n","            if np.argmax(Yp) in prediction2.argsort()[0][::-1]:\n","                correct += 1\n","            else:\n","                compare_int += 1\n","                if (compare_int >= compare):\n","                    human_count += 1\n","                    break\n","        else:\n","            compare_int += 1\n","            if (compare_int >= compare):\n","                human_count += 1\n","                break\n","            \n","elapsed_time = time.time() - start_time\n","print('elapsed_time: {:.3f}s'.format(elapsed_time))\n","print(\"total : %d\" % total)\n","accu = human_count/len(man_)*100\n","print(\"accuracy : %f\" % accu)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jdTntIRyT7H"},"source":["## Any Insights and Future Work"]},{"cell_type":"markdown","source":[" We made predictions through two models. The first is request uri and the second is prediction through time differences between logs. Human pattern analysis was performed through these two, and as a result of prediction through the generated model, it was found that the accuracy exceeded 90%. Through this, it can be seen that web robots and humans show distinctly different pattern differences in log approaches, and these patterns can be learned and predicted. By using this approach, web robots that access specific sites can be sufficiently prevented. This is expected to prevent bad bots trying to achieve personal benefits through data within the company, and to be a sufficient solution for companies with traffic problems or data problems by web robots.<br>"],"metadata":{"id":"kf39XxSpYe7x"}},{"cell_type":"markdown","metadata":{"id":"1iNxstlgmSFS"},"source":["##- Reference\n","\n","[1] Web Robot survey https://ppcprotect.com/blog/ad-fraud/how-many-of-the-internets-users-are-robots/ <br>\n","[2] Lagopoulos, Athanasios and Tsoumakas, Grigorios. (2019). Web robot detection - Server logs [Data set]. Zenodo. https://doi.org/10.5281/zenodo.3477932.<br>\n","[3] Du, Min, et al. \"Deeplog: Anomaly detection and diagnosis from system logs through deep learning.\" Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017.\n","<br>\n","[4] Shinil Kwon, Young-Gab Kim, Sungdeok Cha \"Web robot detection based on pattern-matching technique\" Proceedings of the 2012 SAGE journals. 2012\n","<br>[5] Du, Min, et al. \"LightLog: A lightweight temporal convolutional network for log anomaly detection on the edge.\" Proceedings of the 2021 Science Direct. 2021\n","<br>[6] C. Kim, M. Jang, S. Seo, K. Park and P. Kang, \"Intrusion Detection Based on Sequential Information Preserving Log Embedding Methods and Anomaly Detection Algorithms,\" in IEEE Access, vol. 9, pp. 58088-58101, 2021, doi: 10.1109/ACCESS.2021.3071763.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H2EBkmy4ya0v"},"source":["##- Member's contribution statement"]},{"cell_type":"markdown","source":["### Min-seon Kim\n","Topic Selecion, Reference survey, Pre-processing and Network Modeling Pipeline, Presentor\n","### Yong-Hoon Lee\n","Data Search, Pre-processing and presentation material, Reference survey, Presentor"],"metadata":{"id":"hS-O5dBsoNVo"}},{"cell_type":"markdown","metadata":{"id":"l5UvG9J-yjDs"},"source":["##- Debugging experience worth sharing"]},{"cell_type":"markdown","source":["There was a difficulty in the process of adjusting the hyper-parameter for optimal performance. such as window size, candidtaes, num_params and etc.\n","\n","In the process of making two different models using different data, it took a lot of effort to adjust and fit the format of input features the model.\n","\n","In the process of constructing a workflow that integrates the two log key models and parameter models, it was necessary to redefine the workflow configuration.\n","\n","In the data preprocessing process, custom logic was required in the encoding and train test split which has been explained during the presention.\n","\n"],"metadata":{"id":"Ks-qzHHaoPok"}},{"cell_type":"markdown","metadata":{"id":"KPjQRkRayoRJ"},"source":["##- The Github repository with the commit history"]},{"cell_type":"markdown","source":["https://github.com/sperospera1225/WebRobotDetection"],"metadata":{"id":"CN1ICpCELZBi"}}]}